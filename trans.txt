pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scikit-learn

---------------------------------------------------------------------------
TextCNN
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model TextCNN --word True
Loading data...
Vocab size: 10002
447487it [00:04, 92651.05it/s]
95906it [00:01, 87664.66it/s]
96002it [00:00, 114172.81it/s]
Time usage: 0:00:07
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=6, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   1.8,  Train Acc: 10.94%,  Val Loss:   2.4,  Val Acc: 27.08%,  Time: 0:00:04 *
Iter:    100,  Train Loss:  0.75,  Train Acc: 74.22%,  Val Loss:  0.64,  Val Acc: 77.63%,  Time: 0:00:05 *
Iter:    200,  Train Loss:  0.62,  Train Acc: 76.56%,  Val Loss:  0.58,  Val Acc: 79.62%,  Time: 0:00:07 *
Iter:    300,  Train Loss:  0.54,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 81.04%,  Time: 0:00:08 *
Iter:    400,  Train Loss:  0.47,  Train Acc: 83.59%,  Val Loss:  0.53,  Val Acc: 81.73%,  Time: 0:00:09 *
Iter:    500,  Train Loss:  0.67,  Train Acc: 80.47%,  Val Loss:  0.53,  Val Acc: 81.78%,  Time: 0:00:11 *
Iter:    600,  Train Loss:  0.68,  Train Acc: 74.22%,  Val Loss:  0.51,  Val Acc: 82.33%,  Time: 0:00:12 *
Iter:    700,  Train Loss:  0.61,  Train Acc: 80.47%,  Val Loss:  0.51,  Val Acc: 82.59%,  Time: 0:00:14 *
Iter:    800,  Train Loss:  0.59,  Train Acc: 78.91%,  Val Loss:  0.51,  Val Acc: 82.14%,  Time: 0:00:15
Iter:    900,  Train Loss:  0.53,  Train Acc: 76.56%,  Val Loss:  0.49,  Val Acc: 82.80%,  Time: 0:00:17 *
Iter:   1000,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:  0.52,  Val Acc: 82.03%,  Time: 0:00:18
Iter:   1100,  Train Loss:  0.62,  Train Acc: 78.12%,  Val Loss:  0.51,  Val Acc: 82.46%,  Time: 0:00:20
Iter:   1200,  Train Loss:  0.59,  Train Acc: 78.91%,  Val Loss:  0.49,  Val Acc: 82.90%,  Time: 0:00:21 *
Iter:   1300,  Train Loss:  0.47,  Train Acc: 82.03%,  Val Loss:  0.48,  Val Acc: 83.18%,  Time: 0:00:23 *
Iter:   1400,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.49,  Val Acc: 83.07%,  Time: 0:00:24
Iter:   1500,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 83.59%,  Time: 0:00:26 *
Iter:   1600,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.48,  Val Acc: 83.33%,  Time: 0:00:27
Iter:   1700,  Train Loss:  0.55,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 83.20%,  Time: 0:00:29
Iter:   1800,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 83.36%,  Time: 0:00:30 *
Iter:   1900,  Train Loss:  0.62,  Train Acc: 78.91%,  Val Loss:  0.47,  Val Acc: 83.62%,  Time: 0:00:31 *
Iter:   2000,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.48,  Val Acc: 83.40%,  Time: 0:00:33
Iter:   2100,  Train Loss:  0.45,  Train Acc: 87.50%,  Val Loss:  0.47,  Val Acc: 83.74%,  Time: 0:00:34
Iter:   2200,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.47,  Val Acc: 83.79%,  Time: 0:00:36 *
Iter:   2300,  Train Loss:  0.56,  Train Acc: 81.25%,  Val Loss:  0.48,  Val Acc: 83.37%,  Time: 0:00:37
Iter:   2400,  Train Loss:  0.45,  Train Acc: 82.03%,  Val Loss:  0.48,  Val Acc: 83.39%,  Time: 0:00:39
Iter:   2500,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 83.70%,  Time: 0:00:40
Iter:   2600,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.59%,  Time: 0:00:42
Iter:   2700,  Train Loss:  0.51,  Train Acc: 80.47%,  Val Loss:  0.47,  Val Acc: 83.76%,  Time: 0:00:43
Iter:   2800,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.47,  Val Acc: 83.69%,  Time: 0:00:45 *
Iter:   2900,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.46,  Val Acc: 83.96%,  Time: 0:00:46 *
Iter:   3000,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:  0.47,  Val Acc: 83.63%,  Time: 0:00:48
Iter:   3100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 83.77%,  Time: 0:00:49
Iter:   3200,  Train Loss:  0.55,  Train Acc: 80.47%,  Val Loss:  0.47,  Val Acc: 83.67%,  Time: 0:00:51
Iter:   3300,  Train Loss:  0.54,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.83%,  Time: 0:00:52 *
Iter:   3400,  Train Loss:  0.39,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 84.06%,  Time: 0:00:54
Epoch [2/20]
Iter:   3500,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.73%,  Time: 0:00:55
Iter:   3600,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.46,  Val Acc: 83.69%,  Time: 0:00:57
Iter:   3700,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.46,  Val Acc: 83.93%,  Time: 0:00:58
Iter:   3800,  Train Loss:  0.54,  Train Acc: 77.34%,  Val Loss:  0.46,  Val Acc: 84.15%,  Time: 0:00:59 *
Iter:   3900,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 84.12%,  Time: 0:01:01 *
Iter:   4000,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.92%,  Time: 0:01:02
Iter:   4100,  Train Loss:  0.53,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 83.88%,  Time: 0:01:04
Iter:   4200,  Train Loss:  0.56,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 84.09%,  Time: 0:01:05
Iter:   4300,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 83.97%,  Time: 0:01:07
Iter:   4400,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 83.95%,  Time: 0:01:08
Iter:   4500,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 83.85%,  Time: 0:01:10
Iter:   4600,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 83.91%,  Time: 0:01:11
Iter:   4700,  Train Loss:  0.57,  Train Acc: 79.69%,  Val Loss:  0.46,  Val Acc: 83.97%,  Time: 0:01:13
Iter:   4800,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 84.05%,  Time: 0:01:14
Iter:   4900,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.46,  Val Acc: 84.12%,  Time: 0:01:16
No optimization for a long time, auto-stopping...
Test Loss:  0.45,  Test Acc: 84.34%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8210    0.7878    0.8040     12440
          内科     0.8133    0.8586    0.8354     26267
          外科     0.7328    0.6989    0.7154     14088
         妇产科     0.9259    0.9326    0.9292     21086
          男科     0.8938    0.9000    0.8969     12374
         肿瘤科     0.8663    0.8179    0.8414      9747

    accuracy                         0.8434     96002
   macro avg     0.8422    0.8326    0.8371     96002
weighted avg     0.8430    0.8434    0.8429     96002

Confusion Matrix...
[[ 9800  1731   534   236    61    78]
 [ 1264 22553  1450   466   146   388]
 [  532  2008  9846   433   675   594]
 [  167   445   330 19665   356   123]
 [   48   247   676   220 11136    47]
 [  126   745   600   219    85  7972]]
Time usage: 0:00:01

进程已结束，退出代码为 0


TextCNN	91.22%	Kim 2014 经典的CNN文本分类
TextRNN	91.12%	BiLSTM
TextRNN_Att	90.90%	BiLSTM+Attention
TextRCNN	91.54%	BiLSTM+池化
FastText	92.23%	bow+bigram+trigram， 效果出奇的好
DPCNN	91.25%	深层金字塔CNN
Transformer	89.91%	效果较差
bert	94.83%	bert + fc
ERNIE

---------------------------------------------------------------------------
TextRNN
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model TextRNN --word True
Loading data...
Vocab size: 10002
447487it [00:05, 88669.15it/s]
95906it [00:01, 85272.26it/s]
96002it [00:00, 107211.11it/s]
Time usage: 0:00:07
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300)
  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (fc): Linear(in_features=256, out_features=6, bias=True)
)>
Epoch [1/10]
Iter:      0,  Train Loss:   1.8,  Train Acc: 14.84%,  Val Loss:   1.7,  Val Acc: 26.99%,  Time: 0:00:02 *
Iter:    100,  Train Loss:   1.7,  Train Acc: 24.22%,  Val Loss:   1.7,  Val Acc: 27.09%,  Time: 0:00:04 *
Iter:    200,  Train Loss:   1.1,  Train Acc: 51.56%,  Val Loss:   1.1,  Val Acc: 55.95%,  Time: 0:00:06 *
Iter:    300,  Train Loss:  0.77,  Train Acc: 74.22%,  Val Loss:  0.75,  Val Acc: 75.00%,  Time: 0:00:08 *
Iter:    400,  Train Loss:  0.52,  Train Acc: 85.16%,  Val Loss:  0.64,  Val Acc: 79.20%,  Time: 0:00:10 *
Iter:    500,  Train Loss:  0.62,  Train Acc: 80.47%,  Val Loss:  0.61,  Val Acc: 80.86%,  Time: 0:00:11 *
Iter:    600,  Train Loss:  0.69,  Train Acc: 78.12%,  Val Loss:  0.57,  Val Acc: 81.32%,  Time: 0:00:13 *
Iter:    700,  Train Loss:   0.7,  Train Acc: 77.34%,  Val Loss:  0.55,  Val Acc: 81.77%,  Time: 0:00:15 *
Iter:    800,  Train Loss:  0.69,  Train Acc: 77.34%,  Val Loss:  0.54,  Val Acc: 81.85%,  Time: 0:00:17 *
Iter:    900,  Train Loss:  0.58,  Train Acc: 78.12%,  Val Loss:  0.52,  Val Acc: 82.26%,  Time: 0:00:19 *
Iter:   1000,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.53,  Val Acc: 81.70%,  Time: 0:00:21
Iter:   1100,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.52,  Val Acc: 82.34%,  Time: 0:00:23 *
Iter:   1200,  Train Loss:  0.61,  Train Acc: 80.47%,  Val Loss:   0.5,  Val Acc: 82.72%,  Time: 0:00:25 *
Iter:   1300,  Train Loss:  0.38,  Train Acc: 85.94%,  Val Loss:   0.5,  Val Acc: 82.64%,  Time: 0:00:27 *
Iter:   1400,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.49,  Val Acc: 82.81%,  Time: 0:00:29 *
Iter:   1500,  Train Loss:  0.37,  Train Acc: 85.94%,  Val Loss:  0.49,  Val Acc: 82.81%,  Time: 0:00:31 *
Iter:   1600,  Train Loss:  0.61,  Train Acc: 78.12%,  Val Loss:  0.48,  Val Acc: 83.28%,  Time: 0:00:33 *
Iter:   1700,  Train Loss:  0.54,  Train Acc: 82.81%,  Val Loss:  0.48,  Val Acc: 83.44%,  Time: 0:00:35 *
Iter:   1800,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.37%,  Time: 0:00:36 *
Iter:   1900,  Train Loss:  0.57,  Train Acc: 80.47%,  Val Loss:  0.47,  Val Acc: 83.42%,  Time: 0:00:38
Iter:   2000,  Train Loss:  0.41,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 83.34%,  Time: 0:00:40
Iter:   2100,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.64%,  Time: 0:00:42 *
Iter:   2200,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.46,  Val Acc: 83.82%,  Time: 0:00:44 *
Iter:   2300,  Train Loss:  0.52,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 83.40%,  Time: 0:00:46
Iter:   2400,  Train Loss:  0.43,  Train Acc: 83.59%,  Val Loss:  0.46,  Val Acc: 83.93%,  Time: 0:00:48 *
Iter:   2500,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 83.67%,  Time: 0:00:50
Iter:   2600,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 83.79%,  Time: 0:00:52
Iter:   2700,  Train Loss:  0.55,  Train Acc: 80.47%,  Val Loss:  0.46,  Val Acc: 84.00%,  Time: 0:00:54 *
Iter:   2800,  Train Loss:  0.58,  Train Acc: 79.69%,  Val Loss:  0.45,  Val Acc: 84.05%,  Time: 0:00:56 *
Iter:   2900,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 84.06%,  Time: 0:00:58 *
Iter:   3000,  Train Loss:  0.54,  Train Acc: 81.25%,  Val Loss:  0.45,  Val Acc: 83.95%,  Time: 0:00:59
Iter:   3100,  Train Loss:  0.46,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 83.76%,  Time: 0:01:01
Iter:   3200,  Train Loss:  0.49,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 84.07%,  Time: 0:01:03 *
Iter:   3300,  Train Loss:  0.61,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 84.15%,  Time: 0:01:05
Iter:   3400,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 84.12%,  Time: 0:01:07
Epoch [2/10]
Iter:   3500,  Train Loss:  0.44,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.98%,  Time: 0:01:09
Iter:   3600,  Train Loss:   0.4,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 84.14%,  Time: 0:01:11 *
Iter:   3700,  Train Loss:   0.4,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 84.26%,  Time: 0:01:13 *
Iter:   3800,  Train Loss:  0.55,  Train Acc: 80.47%,  Val Loss:  0.44,  Val Acc: 84.37%,  Time: 0:01:15
Iter:   3900,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.23%,  Time: 0:01:17
Iter:   4000,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 84.14%,  Time: 0:01:19
Iter:   4100,  Train Loss:  0.44,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 84.13%,  Time: 0:01:21
Iter:   4200,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.25%,  Time: 0:01:23 *
Iter:   4300,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 84.14%,  Time: 0:01:24
Iter:   4400,  Train Loss:   0.5,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.48%,  Time: 0:01:26 *
Iter:   4500,  Train Loss:  0.36,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 83.89%,  Time: 0:01:28
Iter:   4600,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 84.29%,  Time: 0:01:30
Iter:   4700,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 84.30%,  Time: 0:01:32 *
Iter:   4800,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 83.98%,  Time: 0:01:34
Iter:   4900,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.45%,  Time: 0:01:36
Iter:   5000,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.08%,  Time: 0:01:38
Iter:   5100,  Train Loss:  0.44,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.24%,  Time: 0:01:40
Iter:   5200,  Train Loss:  0.64,  Train Acc: 79.69%,  Val Loss:  0.44,  Val Acc: 84.41%,  Time: 0:01:42
Iter:   5300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.40%,  Time: 0:01:44 *
Iter:   5400,  Train Loss:  0.38,  Train Acc: 83.59%,  Val Loss:  0.43,  Val Acc: 84.50%,  Time: 0:01:46 *
Iter:   5500,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 84.58%,  Time: 0:01:48
Iter:   5600,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.34%,  Time: 0:01:49
Iter:   5700,  Train Loss:  0.41,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 84.56%,  Time: 0:01:51
Iter:   5800,  Train Loss:  0.33,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 83.77%,  Time: 0:01:53
Iter:   5900,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.43,  Val Acc: 84.58%,  Time: 0:01:55
Iter:   6000,  Train Loss:  0.37,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.43%,  Time: 0:01:57
Iter:   6100,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 84.57%,  Time: 0:01:59
Iter:   6200,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.56%,  Time: 0:02:01
Iter:   6300,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.43,  Val Acc: 84.66%,  Time: 0:02:03
Iter:   6400,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.44,  Val Acc: 84.55%,  Time: 0:02:05
No optimization for a long time, auto-stopping...
Test Loss:  0.43,  Test Acc: 84.66%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8112    0.8029    0.8070     12440
          内科     0.8281    0.8503    0.8390     26267
          外科     0.7560    0.6836    0.7180     14088
         妇产科     0.9165    0.9412    0.9287     21086
          男科     0.8920    0.9082    0.9000     12374
         肿瘤科     0.8475    0.8456    0.8465      9747

    accuracy                         0.8466     96002
   macro avg     0.8419    0.8386    0.8399     96002
weighted avg     0.8449    0.8466    0.8454     96002

Confusion Matrix...
[[ 9988  1619   436   248    59    90]
 [ 1315 22334  1420   570   178   450]
 [  646  1828  9631   525   674   784]
 [  175   344   250 19847   351   119]
 [   56   188   612   240 11238    40]
 [  133   658   390   225    99  8242]]
Time usage: 0:00:02

进程已结束，退出代码为 0

---------------------------------------------------------------------------
TextRNN_Att
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model TextRNN_Att --word True
Loading data...
Vocab size: 10002
447487it [00:05, 87593.95it/s]
95906it [00:01, 83308.43it/s]
96002it [00:00, 108365.35it/s]
Time usage: 0:00:07
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300)
  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (tanh1): Tanh()
  (tanh2): Tanh()
  (fc1): Linear(in_features=256, out_features=64, bias=True)
  (fc): Linear(in_features=64, out_features=6, bias=True)
)>
Epoch [1/10]
Iter:      0,  Train Loss:   1.8,  Train Acc: 11.72%,  Val Loss:   1.8,  Val Acc: 27.08%,  Time: 0:00:02 *
Iter:    100,  Train Loss:  0.76,  Train Acc: 73.44%,  Val Loss:  0.74,  Val Acc: 73.88%,  Time: 0:00:04 *
Iter:    200,  Train Loss:  0.62,  Train Acc: 77.34%,  Val Loss:  0.59,  Val Acc: 79.32%,  Time: 0:00:06 *
Iter:    300,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.55,  Val Acc: 80.88%,  Time: 0:00:08 *
Iter:    400,  Train Loss:  0.47,  Train Acc: 83.59%,  Val Loss:  0.54,  Val Acc: 81.26%,  Time: 0:00:10 *
Iter:    500,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:  0.53,  Val Acc: 81.81%,  Time: 0:00:12 *
Iter:    600,  Train Loss:  0.67,  Train Acc: 80.47%,  Val Loss:  0.51,  Val Acc: 82.09%,  Time: 0:00:14 *
Iter:    700,  Train Loss:  0.64,  Train Acc: 78.12%,  Val Loss:   0.5,  Val Acc: 82.49%,  Time: 0:00:16 *
Iter:    800,  Train Loss:  0.56,  Train Acc: 81.25%,  Val Loss:  0.49,  Val Acc: 82.48%,  Time: 0:00:18 *
Iter:    900,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 82.91%,  Time: 0:00:20 *
Iter:   1000,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.49,  Val Acc: 82.46%,  Time: 0:00:22
Iter:   1100,  Train Loss:  0.58,  Train Acc: 79.69%,  Val Loss:  0.49,  Val Acc: 82.71%,  Time: 0:00:24
Iter:   1200,  Train Loss:  0.53,  Train Acc: 79.69%,  Val Loss:  0.47,  Val Acc: 83.26%,  Time: 0:00:26 *
Iter:   1300,  Train Loss:  0.42,  Train Acc: 82.81%,  Val Loss:  0.48,  Val Acc: 82.88%,  Time: 0:00:28
Iter:   1400,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.47,  Val Acc: 83.51%,  Time: 0:00:30 *
Iter:   1500,  Train Loss:  0.36,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 83.54%,  Time: 0:00:32 *
Iter:   1600,  Train Loss:  0.54,  Train Acc: 80.47%,  Val Loss:  0.47,  Val Acc: 83.34%,  Time: 0:00:34
Iter:   1700,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 83.74%,  Time: 0:00:36 *
Iter:   1800,  Train Loss:   0.4,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 83.72%,  Time: 0:00:38 *
Iter:   1900,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 84.08%,  Time: 0:00:40 *
Iter:   2000,  Train Loss:   0.4,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 83.93%,  Time: 0:00:42
Iter:   2100,  Train Loss:  0.39,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 84.01%,  Time: 0:00:44
Iter:   2200,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 84.04%,  Time: 0:00:46 *
Iter:   2300,  Train Loss:  0.51,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 83.84%,  Time: 0:00:48
Iter:   2400,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.20%,  Time: 0:00:50 *
Iter:   2500,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 84.02%,  Time: 0:00:52
Iter:   2600,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 84.03%,  Time: 0:00:54
Iter:   2700,  Train Loss:  0.46,  Train Acc: 83.59%,  Val Loss:  0.45,  Val Acc: 84.03%,  Time: 0:00:55
Iter:   2800,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 84.24%,  Time: 0:00:57 *
Iter:   2900,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 84.34%,  Time: 0:00:59 *
Iter:   3000,  Train Loss:  0.54,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.12%,  Time: 0:01:01
Iter:   3100,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.31%,  Time: 0:01:03 *
Iter:   3200,  Train Loss:   0.5,  Train Acc: 75.78%,  Val Loss:  0.44,  Val Acc: 84.15%,  Time: 0:01:05
Iter:   3300,  Train Loss:  0.57,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.13%,  Time: 0:01:07
Iter:   3400,  Train Loss:  0.34,  Train Acc: 92.19%,  Val Loss:  0.43,  Val Acc: 84.43%,  Time: 0:01:09 *
Epoch [2/10]
Iter:   3500,  Train Loss:  0.45,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.31%,  Time: 0:01:11
Iter:   3600,  Train Loss:  0.41,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.17%,  Time: 0:01:13
Iter:   3700,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.38%,  Time: 0:01:15
Iter:   3800,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 84.48%,  Time: 0:01:17
Iter:   3900,  Train Loss:  0.46,  Train Acc: 82.03%,  Val Loss:  0.43,  Val Acc: 84.62%,  Time: 0:01:19 *
Iter:   4000,  Train Loss:  0.34,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.60%,  Time: 0:01:21
Iter:   4100,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 84.26%,  Time: 0:01:23
Iter:   4200,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.44,  Val Acc: 84.31%,  Time: 0:01:25
Iter:   4300,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.43,  Val Acc: 84.33%,  Time: 0:01:27
Iter:   4400,  Train Loss:  0.48,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 84.17%,  Time: 0:01:29
Iter:   4500,  Train Loss:  0.36,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 84.16%,  Time: 0:01:31
Iter:   4600,  Train Loss:  0.38,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.12%,  Time: 0:01:33
Iter:   4700,  Train Loss:  0.47,  Train Acc: 82.81%,  Val Loss:  0.43,  Val Acc: 84.38%,  Time: 0:01:35
Iter:   4800,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 84.39%,  Time: 0:01:37
Iter:   4900,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 84.61%,  Time: 0:01:39
No optimization for a long time, auto-stopping...
Test Loss:  0.43,  Test Acc: 84.87%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8169    0.7995    0.8081     12440
          内科     0.8318    0.8530    0.8423     26267
          外科     0.7431    0.7058    0.7240     14088
         妇产科     0.9268    0.9391    0.9329     21086
          男科     0.8987    0.8999    0.8993     12374
         肿瘤科     0.8451    0.8457    0.8454      9747

    accuracy                         0.8487     96002
   macro avg     0.8437    0.8405    0.8420     96002
weighted avg     0.8477    0.8487    0.8481     96002

Confusion Matrix...
[[ 9946  1486   603   202    85   118]
 [ 1365 22407  1396   464   164   471]
 [  501  1852  9943   475   581   736]
 [  202   357   259 19801   336   131]
 [   49   178   740   217 11135    55]
 [  112   659   439   205    89  8243]]
Time usage: 0:00:02

进程已结束，退出代码为 0

---------------------------------------------------------------------------
TextRCNN
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model TextRCNN --word True
Loading data...
Vocab size: 10002
447487it [00:05, 88588.29it/s]
95906it [00:01, 84291.48it/s]
96002it [00:00, 112017.62it/s]
/home/hello/anaconda3/envs/Pytorch/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1.0 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Time usage: 0:00:07
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300)
  (lstm): LSTM(300, 256, batch_first=True, dropout=1.0, bidirectional=True)
  (maxpool): MaxPool1d(kernel_size=32, stride=32, padding=0, dilation=1, ceil_mode=False)
  (fc): Linear(in_features=812, out_features=6, bias=True)
)>
Epoch [1/10]
Iter:      0,  Train Loss:   2.1,  Train Acc: 14.06%,  Val Loss:   1.8,  Val Acc: 13.07%,  Time: 0:00:02 *
Iter:    100,  Train Loss:  0.69,  Train Acc: 77.34%,  Val Loss:  0.65,  Val Acc: 76.95%,  Time: 0:00:05 *
Iter:    200,  Train Loss:  0.59,  Train Acc: 82.03%,  Val Loss:  0.57,  Val Acc: 80.23%,  Time: 0:00:07 *
Iter:    300,  Train Loss:   0.5,  Train Acc: 84.38%,  Val Loss:  0.53,  Val Acc: 81.30%,  Time: 0:00:09 *
Iter:    400,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.53,  Val Acc: 81.93%,  Time: 0:00:12 *
Iter:    500,  Train Loss:  0.57,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 82.35%,  Time: 0:00:14 *
Iter:    600,  Train Loss:  0.66,  Train Acc: 77.34%,  Val Loss:  0.49,  Val Acc: 82.69%,  Time: 0:00:17 *
Iter:    700,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.49,  Val Acc: 82.88%,  Time: 0:00:19 *
Iter:    800,  Train Loss:  0.58,  Train Acc: 76.56%,  Val Loss:  0.51,  Val Acc: 81.93%,  Time: 0:00:21
Iter:    900,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.47,  Val Acc: 83.32%,  Time: 0:00:24 *
Iter:   1000,  Train Loss:   0.5,  Train Acc: 82.81%,  Val Loss:  0.49,  Val Acc: 82.70%,  Time: 0:00:26
Iter:   1100,  Train Loss:  0.58,  Train Acc: 78.91%,  Val Loss:  0.47,  Val Acc: 83.29%,  Time: 0:00:29
Iter:   1200,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.49,  Val Acc: 82.68%,  Time: 0:00:31
Iter:   1300,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.21%,  Time: 0:00:33 *
Iter:   1400,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 83.45%,  Time: 0:00:36 *
Iter:   1500,  Train Loss:  0.34,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 83.86%,  Time: 0:00:38 *
Iter:   1600,  Train Loss:  0.59,  Train Acc: 76.56%,  Val Loss:  0.46,  Val Acc: 83.44%,  Time: 0:00:41
Iter:   1700,  Train Loss:   0.5,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 83.90%,  Time: 0:00:43
Iter:   1800,  Train Loss:  0.38,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 84.10%,  Time: 0:00:46 *
Iter:   1900,  Train Loss:  0.53,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 84.29%,  Time: 0:00:48 *
Iter:   2000,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 84.23%,  Time: 0:00:50 *
Iter:   2100,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 83.69%,  Time: 0:00:53
Iter:   2200,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.32%,  Time: 0:00:55 *
Iter:   2300,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 84.15%,  Time: 0:00:58
Iter:   2400,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.21%,  Time: 0:01:00
Iter:   2500,  Train Loss:   0.4,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.34%,  Time: 0:01:02
Iter:   2600,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.48%,  Time: 0:01:05 *
Iter:   2700,  Train Loss:  0.46,  Train Acc: 80.47%,  Val Loss:  0.44,  Val Acc: 84.21%,  Time: 0:01:07
Iter:   2800,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 84.11%,  Time: 0:01:10
Iter:   2900,  Train Loss:  0.31,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.47%,  Time: 0:01:12
Iter:   3000,  Train Loss:  0.53,  Train Acc: 79.69%,  Val Loss:  0.44,  Val Acc: 84.27%,  Time: 0:01:14
Iter:   3100,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.41%,  Time: 0:01:17
Iter:   3200,  Train Loss:  0.51,  Train Acc: 76.56%,  Val Loss:  0.44,  Val Acc: 84.06%,  Time: 0:01:19
Iter:   3300,  Train Loss:  0.56,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 84.46%,  Time: 0:01:22 *
Iter:   3400,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.43,  Val Acc: 84.54%,  Time: 0:01:24
Epoch [2/10]
Iter:   3500,  Train Loss:   0.4,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 84.76%,  Time: 0:01:26 *
Iter:   3600,  Train Loss:  0.38,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 84.19%,  Time: 0:01:29
Iter:   3700,  Train Loss:  0.34,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 84.27%,  Time: 0:01:31
Iter:   3800,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.43,  Val Acc: 84.74%,  Time: 0:01:34 *
Iter:   3900,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 84.49%,  Time: 0:01:36
Iter:   4000,  Train Loss:  0.32,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 84.82%,  Time: 0:01:39 *
Iter:   4100,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 84.61%,  Time: 0:01:41
Iter:   4200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.46%,  Time: 0:01:43
Iter:   4300,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 84.30%,  Time: 0:01:46
Iter:   4400,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 84.58%,  Time: 0:01:48
Iter:   4500,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 84.58%,  Time: 0:01:51
Iter:   4600,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 84.64%,  Time: 0:01:53
Iter:   4700,  Train Loss:  0.46,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 84.02%,  Time: 0:01:55
Iter:   4800,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.43,  Val Acc: 84.77%,  Time: 0:01:58
Iter:   4900,  Train Loss:   0.4,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 84.66%,  Time: 0:02:00
Iter:   5000,  Train Loss:  0.31,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 84.66%,  Time: 0:02:03
No optimization for a long time, auto-stopping...
Test Loss:  0.42,  Test Acc: 84.96%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8368    0.7808    0.8078     12440
          内科     0.8077    0.8832    0.8438     26267
          外科     0.7452    0.7146    0.7296     14088
         妇产科     0.9421    0.9222    0.9320     21086
          男科     0.9009    0.9035    0.9022     12374
         肿瘤科     0.8730    0.8163    0.8437      9747

    accuracy                         0.8496     96002
   macro avg     0.8510    0.8368    0.8432     96002
weighted avg     0.8505    0.8496    0.8493     96002

Confusion Matrix...
[[ 9713  1932   444   191    66    94]
 [  996 23200  1151   362   171   387]
 [  573  2100 10067   280   560   508]
 [  183   512   485 19445   336   125]
 [   39   232   685   195 11180    43]
 [  104   746   677   167    97  7956]]
Time usage: 0:00:02

进程已结束，退出代码为 0

---------------------------------------------------------------------------
FastText
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model FastText --word True
Loading data...
Vocab size: 10002
447487it [00:20, 22012.88it/s]
95906it [00:03, 27314.98it/s]
96002it [00:04, 21789.64it/s]
Time usage: 0:00:28
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (embedding_ngram2): Embedding(250499, 300)
  (embedding_ngram3): Embedding(250499, 300)
  (dropout): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=900, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=6, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.0,  Train Acc: 25.00%,  Val Loss:   1.9,  Val Acc: 15.00%,  Time: 0:00:03 *
Iter:    100,  Train Loss:   1.5,  Train Acc: 44.53%,  Val Loss:   1.4,  Val Acc: 52.94%,  Time: 0:00:08 *
Iter:    200,  Train Loss:   1.2,  Train Acc: 53.91%,  Val Loss:   1.1,  Val Acc: 57.47%,  Time: 0:00:13 *
Iter:    300,  Train Loss:   1.1,  Train Acc: 61.72%,  Val Loss:  0.93,  Val Acc: 69.29%,  Time: 0:00:23 *
Iter:    400,  Train Loss:  0.89,  Train Acc: 71.09%,  Val Loss:  0.86,  Val Acc: 70.09%,  Time: 0:00:31 *
Iter:    500,  Train Loss:  0.98,  Train Acc: 66.41%,  Val Loss:  0.76,  Val Acc: 74.31%,  Time: 0:00:36 *
Iter:    600,  Train Loss:   0.9,  Train Acc: 65.62%,  Val Loss:  0.74,  Val Acc: 75.68%,  Time: 0:00:42 *
Iter:    700,  Train Loss:  0.89,  Train Acc: 72.66%,  Val Loss:  0.69,  Val Acc: 76.77%,  Time: 0:00:47 *
Iter:    800,  Train Loss:  0.88,  Train Acc: 71.88%,  Val Loss:  0.69,  Val Acc: 77.39%,  Time: 0:00:52 *
Iter:    900,  Train Loss:  0.86,  Train Acc: 68.75%,  Val Loss:  0.67,  Val Acc: 76.55%,  Time: 0:00:56 *
Iter:   1000,  Train Loss:   0.8,  Train Acc: 73.44%,  Val Loss:   0.7,  Val Acc: 76.17%,  Time: 0:01:00
Iter:   1100,  Train Loss:  0.79,  Train Acc: 72.66%,  Val Loss:  0.64,  Val Acc: 78.61%,  Time: 0:01:08 *
Iter:   1200,  Train Loss:   0.7,  Train Acc: 75.78%,  Val Loss:  0.63,  Val Acc: 78.44%,  Time: 0:01:17 *
Iter:   1300,  Train Loss:  0.65,  Train Acc: 78.91%,  Val Loss:  0.62,  Val Acc: 78.79%,  Time: 0:01:23 *
Iter:   1400,  Train Loss:  0.62,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.94%,  Time: 0:01:28 *
Iter:   1500,  Train Loss:  0.64,  Train Acc: 77.34%,  Val Loss:   0.6,  Val Acc: 79.46%,  Time: 0:01:32 *
Iter:   1600,  Train Loss:  0.67,  Train Acc: 74.22%,  Val Loss:  0.59,  Val Acc: 80.34%,  Time: 0:01:37 *
Iter:   1700,  Train Loss:  0.85,  Train Acc: 74.22%,  Val Loss:   0.6,  Val Acc: 79.08%,  Time: 0:01:40
Iter:   1800,  Train Loss:  0.49,  Train Acc: 82.03%,  Val Loss:  0.57,  Val Acc: 80.84%,  Time: 0:01:45 *
Iter:   1900,  Train Loss:  0.67,  Train Acc: 75.78%,  Val Loss:  0.56,  Val Acc: 81.00%,  Time: 0:01:50 *
Iter:   2000,  Train Loss:  0.61,  Train Acc: 75.00%,  Val Loss:  0.57,  Val Acc: 80.40%,  Time: 0:01:53
Iter:   2100,  Train Loss:  0.59,  Train Acc: 82.03%,  Val Loss:  0.57,  Val Acc: 80.78%,  Time: 0:01:56
Iter:   2200,  Train Loss:  0.65,  Train Acc: 81.25%,  Val Loss:  0.56,  Val Acc: 80.71%,  Time: 0:02:01 *
Iter:   2300,  Train Loss:  0.73,  Train Acc: 74.22%,  Val Loss:  0.55,  Val Acc: 81.14%,  Time: 0:02:05 *
Iter:   2400,  Train Loss:  0.54,  Train Acc: 79.69%,  Val Loss:  0.54,  Val Acc: 81.58%,  Time: 0:02:10 *
Iter:   2500,  Train Loss:   0.5,  Train Acc: 82.81%,  Val Loss:  0.55,  Val Acc: 80.95%,  Time: 0:02:13
Iter:   2600,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.53,  Val Acc: 81.77%,  Time: 0:02:18 *
Iter:   2700,  Train Loss:  0.58,  Train Acc: 77.34%,  Val Loss:  0.53,  Val Acc: 81.55%,  Time: 0:02:22 *
Iter:   2800,  Train Loss:  0.66,  Train Acc: 78.12%,  Val Loss:  0.52,  Val Acc: 82.03%,  Time: 0:02:27 *
Iter:   2900,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.52,  Val Acc: 81.95%,  Time: 0:02:34 *
Iter:   3000,  Train Loss:  0.56,  Train Acc: 82.81%,  Val Loss:  0.52,  Val Acc: 82.14%,  Time: 0:02:43 *
Iter:   3100,  Train Loss:  0.52,  Train Acc: 82.03%,  Val Loss:  0.53,  Val Acc: 81.87%,  Time: 0:02:46
Iter:   3200,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.52,  Val Acc: 82.10%,  Time: 0:02:49
Iter:   3300,  Train Loss:   0.7,  Train Acc: 78.91%,  Val Loss:  0.52,  Val Acc: 82.05%,  Time: 0:02:54 *
Iter:   3400,  Train Loss:   0.5,  Train Acc: 81.25%,  Val Loss:  0.51,  Val Acc: 82.69%,  Time: 0:03:02 *
Epoch [2/20]
Iter:   3500,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.51,  Val Acc: 82.66%,  Time: 0:03:05
Iter:   3600,  Train Loss:  0.56,  Train Acc: 81.25%,  Val Loss:  0.51,  Val Acc: 82.38%,  Time: 0:03:09
Iter:   3700,  Train Loss:  0.46,  Train Acc: 82.03%,  Val Loss:  0.51,  Val Acc: 82.18%,  Time: 0:03:12
Iter:   3800,  Train Loss:   0.6,  Train Acc: 75.00%,  Val Loss:   0.5,  Val Acc: 82.75%,  Time: 0:03:16 *
Iter:   3900,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.49,  Val Acc: 82.94%,  Time: 0:03:21 *
Iter:   4000,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:   0.5,  Val Acc: 82.57%,  Time: 0:03:24
Iter:   4100,  Train Loss:  0.58,  Train Acc: 82.03%,  Val Loss:  0.51,  Val Acc: 82.45%,  Time: 0:03:28
Iter:   4200,  Train Loss:   0.6,  Train Acc: 79.69%,  Val Loss:   0.5,  Val Acc: 82.70%,  Time: 0:03:31
Iter:   4300,  Train Loss:  0.36,  Train Acc: 87.50%,  Val Loss:  0.49,  Val Acc: 82.95%,  Time: 0:03:36 *
Iter:   4400,  Train Loss:   0.6,  Train Acc: 78.91%,  Val Loss:  0.49,  Val Acc: 82.98%,  Time: 0:03:40 *
Iter:   4500,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 82.97%,  Time: 0:03:43
Iter:   4600,  Train Loss:  0.57,  Train Acc: 80.47%,  Val Loss:   0.5,  Val Acc: 82.78%,  Time: 0:03:47
Iter:   4700,  Train Loss:  0.64,  Train Acc: 77.34%,  Val Loss:  0.48,  Val Acc: 83.29%,  Time: 0:03:51 *
Iter:   4800,  Train Loss:   0.6,  Train Acc: 78.91%,  Val Loss:  0.49,  Val Acc: 82.96%,  Time: 0:03:55
Iter:   4900,  Train Loss:   0.5,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 82.98%,  Time: 0:03:58
Iter:   5000,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 83.32%,  Time: 0:04:01
Iter:   5100,  Train Loss:  0.51,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 83.15%,  Time: 0:04:05
Iter:   5200,  Train Loss:  0.63,  Train Acc: 78.91%,  Val Loss:  0.48,  Val Acc: 83.36%,  Time: 0:04:09 *
Iter:   5300,  Train Loss:  0.48,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 83.43%,  Time: 0:04:14 *
Iter:   5400,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:  0.47,  Val Acc: 83.65%,  Time: 0:04:24 *
Iter:   5500,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.47,  Val Acc: 83.56%,  Time: 0:04:27
Iter:   5600,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 83.72%,  Time: 0:04:32 *
Iter:   5700,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.48,  Val Acc: 83.43%,  Time: 0:04:35
Iter:   5800,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.47,  Val Acc: 83.51%,  Time: 0:04:39
Iter:   5900,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.62%,  Time: 0:04:42
Iter:   6000,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.47,  Val Acc: 83.72%,  Time: 0:04:47 *
Iter:   6100,  Train Loss:  0.51,  Train Acc: 80.47%,  Val Loss:  0.47,  Val Acc: 83.70%,  Time: 0:04:50
Iter:   6200,  Train Loss:  0.45,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 83.75%,  Time: 0:04:55 *
Iter:   6300,  Train Loss:  0.46,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.79%,  Time: 0:05:00 *
Iter:   6400,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.46,  Val Acc: 83.74%,  Time: 0:05:03
Iter:   6500,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.93%,  Time: 0:05:08 *
Iter:   6600,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.46,  Val Acc: 83.94%,  Time: 0:05:11
Iter:   6700,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 83.78%,  Time: 0:05:14
Iter:   6800,  Train Loss:  0.47,  Train Acc: 79.69%,  Val Loss:  0.46,  Val Acc: 83.75%,  Time: 0:05:18
Iter:   6900,  Train Loss:  0.55,  Train Acc: 82.03%,  Val Loss:  0.46,  Val Acc: 83.99%,  Time: 0:05:22 *
Epoch [3/20]
Iter:   7000,  Train Loss:  0.36,  Train Acc: 85.94%,  Val Loss:  0.46,  Val Acc: 84.04%,  Time: 0:05:26
Iter:   7100,  Train Loss:  0.51,  Train Acc: 81.25%,  Val Loss:  0.46,  Val Acc: 83.92%,  Time: 0:05:29
Iter:   7200,  Train Loss:  0.33,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 83.90%,  Time: 0:05:32
Iter:   7300,  Train Loss:   0.4,  Train Acc: 83.59%,  Val Loss:  0.46,  Val Acc: 83.96%,  Time: 0:05:36
Iter:   7400,  Train Loss:  0.36,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 84.13%,  Time: 0:05:40 *
Iter:   7500,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.45,  Val Acc: 84.10%,  Time: 0:05:44
Iter:   7600,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 84.02%,  Time: 0:05:47
Iter:   7700,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.91%,  Time: 0:05:50
Iter:   7800,  Train Loss:  0.56,  Train Acc: 78.91%,  Val Loss:  0.46,  Val Acc: 83.94%,  Time: 0:05:53
Iter:   7900,  Train Loss:   0.6,  Train Acc: 81.25%,  Val Loss:  0.45,  Val Acc: 84.16%,  Time: 0:05:57
Iter:   8000,  Train Loss:  0.46,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.94%,  Time: 0:06:00
Iter:   8100,  Train Loss:  0.42,  Train Acc: 83.59%,  Val Loss:  0.46,  Val Acc: 84.16%,  Time: 0:06:03
Iter:   8200,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.45,  Val Acc: 84.30%,  Time: 0:06:08 *
Iter:   8300,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 84.09%,  Time: 0:06:11
Iter:   8400,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 84.23%,  Time: 0:06:15
Iter:   8500,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 84.33%,  Time: 0:06:18
Iter:   8600,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 84.33%,  Time: 0:06:21
Iter:   8700,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 84.37%,  Time: 0:06:26 *
Iter:   8800,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 84.39%,  Time: 0:06:29
Iter:   8900,  Train Loss:  0.47,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 84.55%,  Time: 0:06:34 *
Iter:   9000,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.45,  Val Acc: 84.47%,  Time: 0:06:38
Iter:   9100,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 84.57%,  Time: 0:06:42 *
Iter:   9200,  Train Loss:  0.43,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 84.41%,  Time: 0:06:46
Iter:   9300,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 84.55%,  Time: 0:06:49
Iter:   9400,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.45,  Val Acc: 84.43%,  Time: 0:06:52
Iter:   9500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.64%,  Time: 0:06:57 *
Iter:   9600,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 84.48%,  Time: 0:07:00
Iter:   9700,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.45,  Val Acc: 84.44%,  Time: 0:07:04
Iter:   9800,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 84.41%,  Time: 0:07:07
Iter:   9900,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.45,  Val Acc: 84.30%,  Time: 0:07:10
Iter:  10000,  Train Loss:  0.37,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 84.57%,  Time: 0:07:15 *
Iter:  10100,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 84.62%,  Time: 0:07:20 *
Iter:  10200,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.44,  Val Acc: 84.60%,  Time: 0:07:23
Iter:  10300,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.44,  Val Acc: 84.66%,  Time: 0:07:26
Iter:  10400,  Train Loss:  0.42,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 84.67%,  Time: 0:07:30
Epoch [4/20]
Iter:  10500,  Train Loss:  0.49,  Train Acc: 81.25%,  Val Loss:  0.44,  Val Acc: 84.73%,  Time: 0:07:34 *
Iter:  10600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 84.65%,  Time: 0:07:38
Iter:  10700,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 84.67%,  Time: 0:07:41
Iter:  10800,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 84.73%,  Time: 0:07:44
Iter:  10900,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:  0.44,  Val Acc: 84.64%,  Time: 0:07:48
Iter:  11000,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 84.77%,  Time: 0:07:51
Iter:  11100,  Train Loss:  0.33,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 84.58%,  Time: 0:07:54
Iter:  11200,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 84.70%,  Time: 0:07:58
Iter:  11300,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.45,  Val Acc: 84.50%,  Time: 0:08:01
Iter:  11400,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.71%,  Time: 0:08:04
Iter:  11500,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 84.68%,  Time: 0:08:07
No optimization for a long time, auto-stopping...
Test Loss:  0.43,  Test Acc: 84.83%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8287    0.7911    0.8095     12440
          内科     0.8096    0.8741    0.8406     26267
          外科     0.7772    0.6819    0.7265     14088
         妇产科     0.9237    0.9353    0.9294     21086
          男科     0.8944    0.9002    0.8973     12374
         肿瘤科     0.8511    0.8383    0.8446      9747

    accuracy                         0.8483     96002
   macro avg     0.8474    0.8368    0.8413     96002
weighted avg     0.8475    0.8483    0.8470     96002

Confusion Matrix...
[[ 9841  1771   401   228    95   104]
 [ 1172 22959   953   532   202   449]
 [  565  2177  9607   407   646   686]
 [  155   465   324 19721   275   146]
 [   43   230   660   257 11139    45]
 [   99   758   416   206    97  8171]]
Time usage: 0:00:01

进程已结束，退出代码为 0

---------------------------------------------------------------------------
DPCNN
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model DPCNN --word True
Loading data...
Vocab size: 10002
447487it [00:05, 88994.37it/s]
95906it [00:01, 85530.57it/s]
96002it [00:00, 107838.40it/s]
Time usage: 0:00:07
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300)
  (conv_region): Conv2d(1, 250, kernel_size=(3, 300), stride=(1, 1))
  (conv): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))
  (max_pool): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)
  (padding1): ZeroPad2d((0, 0, 1, 1))
  (padding2): ZeroPad2d((0, 0, 0, 1))
  (relu): ReLU()
  (fc): Linear(in_features=250, out_features=6, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   1.8,  Train Acc: 16.41%,  Val Loss:   2.9,  Val Acc: 27.08%,  Time: 0:00:04 *
Iter:    100,  Train Loss:  0.88,  Train Acc: 71.09%,  Val Loss:  0.83,  Val Acc: 70.06%,  Time: 0:00:06 *
Iter:    200,  Train Loss:  0.71,  Train Acc: 76.56%,  Val Loss:  0.64,  Val Acc: 77.98%,  Time: 0:00:09 *
Iter:    300,  Train Loss:   0.6,  Train Acc: 77.34%,  Val Loss:  0.59,  Val Acc: 79.58%,  Time: 0:00:12 *
Iter:    400,  Train Loss:  0.51,  Train Acc: 86.72%,  Val Loss:  0.59,  Val Acc: 80.21%,  Time: 0:00:14
Iter:    500,  Train Loss:  0.66,  Train Acc: 83.59%,  Val Loss:  0.56,  Val Acc: 81.35%,  Time: 0:00:17 *
Iter:    600,  Train Loss:   0.7,  Train Acc: 75.78%,  Val Loss:  0.54,  Val Acc: 81.10%,  Time: 0:00:20 *
Iter:    700,  Train Loss:  0.62,  Train Acc: 75.00%,  Val Loss:  0.52,  Val Acc: 81.98%,  Time: 0:00:22 *
Iter:    800,  Train Loss:  0.64,  Train Acc: 77.34%,  Val Loss:  0.54,  Val Acc: 80.96%,  Time: 0:00:25
Iter:    900,  Train Loss:  0.54,  Train Acc: 78.12%,  Val Loss:  0.51,  Val Acc: 82.21%,  Time: 0:00:27 *
Iter:   1000,  Train Loss:  0.53,  Train Acc: 83.59%,  Val Loss:   0.5,  Val Acc: 82.36%,  Time: 0:00:30 *
Iter:   1100,  Train Loss:  0.59,  Train Acc: 78.91%,  Val Loss:  0.49,  Val Acc: 82.69%,  Time: 0:00:32 *
Iter:   1200,  Train Loss:  0.54,  Train Acc: 82.03%,  Val Loss:  0.52,  Val Acc: 81.97%,  Time: 0:00:34
Iter:   1300,  Train Loss:  0.38,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 82.88%,  Time: 0:00:37 *
Iter:   1400,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.51,  Val Acc: 82.35%,  Time: 0:00:39
Iter:   1500,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.48,  Val Acc: 83.08%,  Time: 0:00:41 *
Iter:   1600,  Train Loss:  0.56,  Train Acc: 77.34%,  Val Loss:  0.48,  Val Acc: 83.16%,  Time: 0:00:44
Iter:   1700,  Train Loss:  0.55,  Train Acc: 82.81%,  Val Loss:  0.49,  Val Acc: 82.98%,  Time: 0:00:46
Iter:   1800,  Train Loss:  0.37,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 83.58%,  Time: 0:00:48 *
Iter:   1900,  Train Loss:  0.55,  Train Acc: 82.03%,  Val Loss:  0.46,  Val Acc: 83.94%,  Time: 0:00:51 *
Iter:   2000,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 83.42%,  Time: 0:00:53
Iter:   2100,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.48,  Val Acc: 83.41%,  Time: 0:00:55
Iter:   2200,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.69%,  Time: 0:00:57
Iter:   2300,  Train Loss:  0.49,  Train Acc: 81.25%,  Val Loss:  0.47,  Val Acc: 83.53%,  Time: 0:00:59
Iter:   2400,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 83.49%,  Time: 0:01:02
Iter:   2500,  Train Loss:  0.41,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 83.48%,  Time: 0:01:04
Iter:   2600,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 83.83%,  Time: 0:01:06 *
Iter:   2700,  Train Loss:  0.49,  Train Acc: 79.69%,  Val Loss:  0.47,  Val Acc: 83.34%,  Time: 0:01:08
Iter:   2800,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 83.93%,  Time: 0:01:11 *
Iter:   2900,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 83.96%,  Time: 0:01:13
Iter:   3000,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 83.62%,  Time: 0:01:15
Iter:   3100,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.46,  Val Acc: 83.85%,  Time: 0:01:17
Iter:   3200,  Train Loss:  0.49,  Train Acc: 79.69%,  Val Loss:  0.46,  Val Acc: 83.39%,  Time: 0:01:19
Iter:   3300,  Train Loss:  0.52,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 83.86%,  Time: 0:01:22
Iter:   3400,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.46,  Val Acc: 83.94%,  Time: 0:01:24
Epoch [2/20]
Iter:   3500,  Train Loss:  0.47,  Train Acc: 82.81%,  Val Loss:  0.46,  Val Acc: 84.07%,  Time: 0:01:26
Iter:   3600,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 83.58%,  Time: 0:01:28
Iter:   3700,  Train Loss:   0.4,  Train Acc: 82.81%,  Val Loss:  0.47,  Val Acc: 83.65%,  Time: 0:01:30
Iter:   3800,  Train Loss:   0.5,  Train Acc: 78.91%,  Val Loss:  0.46,  Val Acc: 84.10%,  Time: 0:01:33
No optimization for a long time, auto-stopping...
Test Loss:  0.45,  Test Acc: 83.94%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8543    0.7326    0.7888     12440
          内科     0.7949    0.8811    0.8358     26267
          外科     0.7103    0.7139    0.7121     14088
         妇产科     0.9336    0.9242    0.9288     21086
          男科     0.8896    0.8958    0.8927     12374
         肿瘤科     0.8822    0.7893    0.8332      9747

    accuracy                         0.8394     96002
   macro avg     0.8441    0.8228    0.8319     96002
weighted avg     0.8417    0.8394    0.8391     96002

Confusion Matrix...
[[ 9114  2212   726   220    84    84]
 [  898 23145  1286   423   171   344]
 [  391  2212 10058   290   690   447]
 [  151   475   520 19487   343   110]
 [   29   290   685   243 11085    42]
 [   85   784   886   211    88  7693]]
Time usage: 0:00:02

进程已结束，退出代码为 0

---------------------------------------------------------------------------
Transformer
---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python run.py --model Transformer --word True
Loading data...
Vocab size: 10002
447487it [00:04, 89682.89it/s]
95906it [00:01, 85877.98it/s]
96002it [00:00, 110228.37it/s]
Time usage: 0:00:07
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300)
  (postion_embedding): Positional_Encoding(
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (encoder): Encoder(
    (attention): Multi_Head_Attention(
      (fc_Q): Linear(in_features=300, out_features=300, bias=True)
      (fc_K): Linear(in_features=300, out_features=300, bias=True)
      (fc_V): Linear(in_features=300, out_features=300, bias=True)
      (attention): Scaled_Dot_Product_Attention()
      (fc): Linear(in_features=300, out_features=300, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
    )
    (feed_forward): Position_wise_Feed_Forward(
      (fc1): Linear(in_features=300, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=300, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
    )
  )
  (encoders): ModuleList(
    (0-1): 2 x Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fc1): Linear(in_features=9600, out_features=6, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.0,  Train Acc: 11.72%,  Val Loss:   4.7,  Val Acc: 27.08%,  Time: 0:00:03 *
Iter:    100,  Train Loss:   1.2,  Train Acc: 60.94%,  Val Loss:   1.2,  Val Acc: 62.86%,  Time: 0:00:06 *
Iter:    200,  Train Loss:  0.91,  Train Acc: 66.41%,  Val Loss:  0.92,  Val Acc: 71.93%,  Time: 0:00:09 *
Iter:    300,  Train Loss:  0.75,  Train Acc: 73.44%,  Val Loss:  0.81,  Val Acc: 76.14%,  Time: 0:00:12 *
Iter:    400,  Train Loss:  0.71,  Train Acc: 75.78%,  Val Loss:   0.8,  Val Acc: 77.36%,  Time: 0:00:15 *
Iter:    500,  Train Loss:  0.87,  Train Acc: 71.88%,  Val Loss:  0.77,  Val Acc: 77.68%,  Time: 0:00:17 *
Iter:    600,  Train Loss:  0.99,  Train Acc: 70.31%,  Val Loss:  0.78,  Val Acc: 78.10%,  Time: 0:00:20
Iter:    700,  Train Loss:  0.92,  Train Acc: 71.09%,  Val Loss:  0.73,  Val Acc: 78.87%,  Time: 0:00:23 *
Iter:    800,  Train Loss:  0.81,  Train Acc: 75.78%,  Val Loss:   0.7,  Val Acc: 79.92%,  Time: 0:00:26 *
Iter:    900,  Train Loss:  0.67,  Train Acc: 75.00%,  Val Loss:  0.76,  Val Acc: 79.67%,  Time: 0:00:29
Iter:   1000,  Train Loss:   0.7,  Train Acc: 79.69%,  Val Loss:  0.76,  Val Acc: 79.48%,  Time: 0:00:31
Iter:   1100,  Train Loss:  0.74,  Train Acc: 77.34%,  Val Loss:   0.7,  Val Acc: 80.31%,  Time: 0:00:34
Iter:   1200,  Train Loss:  0.73,  Train Acc: 78.12%,  Val Loss:  0.71,  Val Acc: 80.18%,  Time: 0:00:37
Iter:   1300,  Train Loss:  0.57,  Train Acc: 75.78%,  Val Loss:  0.77,  Val Acc: 78.46%,  Time: 0:00:40
Iter:   1400,  Train Loss:   0.6,  Train Acc: 80.47%,  Val Loss:  0.68,  Val Acc: 80.46%,  Time: 0:00:43 *
Iter:   1500,  Train Loss:  0.54,  Train Acc: 80.47%,  Val Loss:  0.66,  Val Acc: 81.02%,  Time: 0:00:45 *
Iter:   1600,  Train Loss:  0.71,  Train Acc: 77.34%,  Val Loss:  0.61,  Val Acc: 80.92%,  Time: 0:00:48 *
Iter:   1700,  Train Loss:  0.79,  Train Acc: 73.44%,  Val Loss:  0.72,  Val Acc: 79.58%,  Time: 0:00:51
Iter:   1800,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.67,  Val Acc: 81.34%,  Time: 0:00:54
Iter:   1900,  Train Loss:  0.71,  Train Acc: 77.34%,  Val Loss:  0.65,  Val Acc: 80.54%,  Time: 0:00:57
Iter:   2000,  Train Loss:   0.6,  Train Acc: 82.03%,  Val Loss:  0.61,  Val Acc: 81.35%,  Time: 0:00:59 *
Iter:   2100,  Train Loss:  0.54,  Train Acc: 83.59%,  Val Loss:  0.65,  Val Acc: 80.51%,  Time: 0:01:02
Iter:   2200,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.64,  Val Acc: 81.21%,  Time: 0:01:05
Iter:   2300,  Train Loss:  0.73,  Train Acc: 75.78%,  Val Loss:  0.68,  Val Acc: 80.13%,  Time: 0:01:08
Iter:   2400,  Train Loss:  0.62,  Train Acc: 78.12%,  Val Loss:  0.62,  Val Acc: 81.47%,  Time: 0:01:10
Iter:   2500,  Train Loss:  0.51,  Train Acc: 85.94%,  Val Loss:  0.59,  Val Acc: 81.71%,  Time: 0:01:13 *
Iter:   2600,  Train Loss:  0.62,  Train Acc: 82.81%,  Val Loss:  0.64,  Val Acc: 81.56%,  Time: 0:01:16
Iter:   2700,  Train Loss:  0.66,  Train Acc: 75.00%,  Val Loss:  0.56,  Val Acc: 82.28%,  Time: 0:01:19 *
Iter:   2800,  Train Loss:  0.76,  Train Acc: 77.34%,  Val Loss:   0.6,  Val Acc: 82.00%,  Time: 0:01:22
Iter:   2900,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.61,  Val Acc: 81.72%,  Time: 0:01:24
Iter:   3000,  Train Loss:  0.64,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 81.64%,  Time: 0:01:27
Iter:   3100,  Train Loss:  0.55,  Train Acc: 82.81%,  Val Loss:  0.57,  Val Acc: 81.98%,  Time: 0:01:30
Iter:   3200,  Train Loss:  0.56,  Train Acc: 78.91%,  Val Loss:  0.56,  Val Acc: 82.26%,  Time: 0:01:33 *
Iter:   3300,  Train Loss:  0.67,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 82.23%,  Time: 0:01:35
Iter:   3400,  Train Loss:  0.48,  Train Acc: 79.69%,  Val Loss:  0.63,  Val Acc: 80.89%,  Time: 0:01:38
Epoch [2/20]
Iter:   3500,  Train Loss:  0.58,  Train Acc: 81.25%,  Val Loss:  0.57,  Val Acc: 82.40%,  Time: 0:01:41
Iter:   3600,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.57,  Val Acc: 82.16%,  Time: 0:01:44
Iter:   3700,  Train Loss:  0.49,  Train Acc: 82.03%,  Val Loss:  0.57,  Val Acc: 82.27%,  Time: 0:01:47
Iter:   3800,  Train Loss:  0.68,  Train Acc: 75.00%,  Val Loss:  0.55,  Val Acc: 82.41%,  Time: 0:01:49 *
Iter:   3900,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.58,  Val Acc: 82.59%,  Time: 0:01:52
Iter:   4000,  Train Loss:  0.44,  Train Acc: 85.16%,  Val Loss:  0.59,  Val Acc: 82.41%,  Time: 0:01:55
Iter:   4100,  Train Loss:  0.56,  Train Acc: 82.03%,  Val Loss:  0.54,  Val Acc: 82.38%,  Time: 0:01:58 *
Iter:   4200,  Train Loss:  0.58,  Train Acc: 79.69%,  Val Loss:  0.53,  Val Acc: 82.83%,  Time: 0:02:00 *
Iter:   4300,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.55,  Val Acc: 82.84%,  Time: 0:02:03
Iter:   4400,  Train Loss:  0.68,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 81.82%,  Time: 0:02:06
Iter:   4500,  Train Loss:  0.49,  Train Acc: 82.03%,  Val Loss:  0.56,  Val Acc: 82.67%,  Time: 0:02:09
Iter:   4600,  Train Loss:   0.5,  Train Acc: 82.81%,  Val Loss:  0.56,  Val Acc: 82.72%,  Time: 0:02:12
Iter:   4700,  Train Loss:  0.63,  Train Acc: 80.47%,  Val Loss:  0.56,  Val Acc: 81.99%,  Time: 0:02:14
Iter:   4800,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.57,  Val Acc: 82.27%,  Time: 0:02:17
Iter:   4900,  Train Loss:  0.63,  Train Acc: 77.34%,  Val Loss:  0.56,  Val Acc: 82.42%,  Time: 0:02:20
Iter:   5000,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.54,  Val Acc: 82.91%,  Time: 0:02:23
Iter:   5100,  Train Loss:  0.55,  Train Acc: 82.03%,  Val Loss:  0.55,  Val Acc: 82.92%,  Time: 0:02:25
Iter:   5200,  Train Loss:  0.82,  Train Acc: 74.22%,  Val Loss:  0.56,  Val Acc: 82.32%,  Time: 0:02:28
Iter:   5300,  Train Loss:  0.62,  Train Acc: 78.91%,  Val Loss:  0.55,  Val Acc: 82.87%,  Time: 0:02:31
Iter:   5400,  Train Loss:  0.66,  Train Acc: 76.56%,  Val Loss:  0.55,  Val Acc: 82.73%,  Time: 0:02:34
Iter:   5500,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.58,  Val Acc: 82.20%,  Time: 0:02:36
Iter:   5600,  Train Loss:  0.56,  Train Acc: 79.69%,  Val Loss:  0.55,  Val Acc: 82.85%,  Time: 0:02:39
Iter:   5700,  Train Loss:  0.47,  Train Acc: 83.59%,  Val Loss:  0.55,  Val Acc: 82.82%,  Time: 0:02:42
Iter:   5800,  Train Loss:  0.48,  Train Acc: 79.69%,  Val Loss:  0.52,  Val Acc: 82.67%,  Time: 0:02:45 *
Iter:   5900,  Train Loss:  0.34,  Train Acc: 85.94%,  Val Loss:  0.57,  Val Acc: 82.76%,  Time: 0:02:47
Iter:   6000,  Train Loss:  0.61,  Train Acc: 78.12%,  Val Loss:  0.54,  Val Acc: 82.79%,  Time: 0:02:50
Iter:   6100,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 82.97%,  Time: 0:02:53
Iter:   6200,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.53,  Val Acc: 83.07%,  Time: 0:02:56
Iter:   6300,  Train Loss:  0.47,  Train Acc: 82.03%,  Val Loss:  0.55,  Val Acc: 82.80%,  Time: 0:02:58
Iter:   6400,  Train Loss:  0.66,  Train Acc: 77.34%,  Val Loss:  0.56,  Val Acc: 82.70%,  Time: 0:03:01
Iter:   6500,  Train Loss:  0.52,  Train Acc: 79.69%,  Val Loss:  0.54,  Val Acc: 82.84%,  Time: 0:03:04
Iter:   6600,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.54,  Val Acc: 83.05%,  Time: 0:03:07
Iter:   6700,  Train Loss:  0.48,  Train Acc: 86.72%,  Val Loss:  0.54,  Val Acc: 83.05%,  Time: 0:03:09
Iter:   6800,  Train Loss:  0.54,  Train Acc: 78.12%,  Val Loss:  0.55,  Val Acc: 83.01%,  Time: 0:03:12
Iter:   6900,  Train Loss:  0.57,  Train Acc: 78.12%,  Val Loss:  0.54,  Val Acc: 82.89%,  Time: 0:03:15
Epoch [3/20]
Iter:   7000,  Train Loss:  0.41,  Train Acc: 87.50%,  Val Loss:  0.54,  Val Acc: 83.18%,  Time: 0:03:18
Iter:   7100,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.54,  Val Acc: 82.97%,  Time: 0:03:21
Iter:   7200,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.55,  Val Acc: 83.03%,  Time: 0:03:23
Iter:   7300,  Train Loss:  0.53,  Train Acc: 85.16%,  Val Loss:  0.54,  Val Acc: 83.03%,  Time: 0:03:26
Iter:   7400,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.56,  Val Acc: 82.99%,  Time: 0:03:29
Iter:   7500,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.53,  Val Acc: 83.10%,  Time: 0:03:32
Iter:   7600,  Train Loss:  0.45,  Train Acc: 82.81%,  Val Loss:  0.53,  Val Acc: 82.93%,  Time: 0:03:34
Iter:   7700,  Train Loss:  0.59,  Train Acc: 77.34%,  Val Loss:  0.53,  Val Acc: 83.02%,  Time: 0:03:37
Iter:   7800,  Train Loss:  0.65,  Train Acc: 76.56%,  Val Loss:  0.55,  Val Acc: 83.32%,  Time: 0:03:40
No optimization for a long time, auto-stopping...
Test Loss:  0.52,  Test Acc: 82.83%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

          儿科     0.8340    0.7428    0.7858     12440
          内科     0.7636    0.8921    0.8229     26267
          外科     0.7507    0.6135    0.6752     14088
         妇产科     0.9365    0.9114    0.9238     21086
          男科     0.8401    0.9248    0.8804     12374
         肿瘤科     0.8793    0.7738    0.8232      9747

    accuracy                         0.8283     96002
   macro avg     0.8340    0.8097    0.8185     96002
weighted avg     0.8304    0.8283    0.8260     96002

Confusion Matrix...
[[ 9241  2360   475   190    97    77]
 [  995 23434   964   364   220   290]
 [  517  2815  8643   325  1276   512]
 [  200   659   427 19217   456   127]
 [   45   324   322   211 11443    29]
 [   83  1098   683   212   129  7542]]
Time usage: 0:00:02

进程已结束，退出代码为 0

---------------------------------------------------------------------------
/home/hello/anaconda3/envs/Pytorch/bin/python /home/hello/xinrenMin/foryang/Chinese-Text-Classification-Pytorch-master/Test1.py
463602
463602
数据加载成功
NB模型保存完成
[LibSVM]........................................*...........*
optimization finished, #iter = 51868
obj = -35144.625205, rho = -0.435472
nSV = 42648, nBSV = 35864
Total nSV = 42648
.........................................*...........*
optimization finished, #iter = 52282
obj = -35384.628209, rho = -0.431928
nSV = 42908, nBSV = 36116
Total nSV = 42908
.........................................*...........*
optimization finished, #iter = 52098
obj = -35187.557599, rho = -0.465514
nSV = 42685, nBSV = 35903
Total nSV = 42685
........................................*...........*
optimization finished, #iter = 51800
obj = -35264.251616, rho = -0.442970
nSV = 42740, nBSV = 36025
Total nSV = 42740
........................................*...........*
optimization finished, #iter = 51392
obj = -35074.858296, rho = -0.439441
nSV = 42441, nBSV = 35741
Total nSV = 42441
..................................................*...............*
optimization finished, #iter = 65323
obj = -43621.600355, rho = 0.435742
nSV = 52540, nBSV = 44551
........................*.......*
optimization finished, #iter = 31655
obj = -22626.255080, rho = -0.110590
nSV = 27964, nBSV = 23506
Total nSV = 27964
........................*.......*
optimization finished, #iter = 31240
obj = -22610.496539, rho = -0.139309
nSV = 27910, nBSV = 23531
Total nSV = 27910
.........................*......*
optimization finished, #iter = 31280
obj = -22725.799645, rho = -0.119024
nSV = 28032, nBSV = 23612
Total nSV = 28032
........................*.......*
optimization finished, #iter = 31012
obj = -22652.856833, rho = -0.107258
nSV = 27921, nBSV = 23550
Total nSV = 27921
........................*.......*
optimization finished, #iter = 31240
obj = -22697.208524, rho = -0.129525
nSV = 28005, nBSV = 23583
Total nSV = 28005
..............................*........*
optimization finished, #iter = 38193
obj = -27932.316326, rho = 0.116536
nSV = 34262, nBSV = 29062
............*....*
optimization finished, #iter = 16229
obj = -10081.952465, rho = -0.122295
nSV = 13604, nBSV = 10201
Total nSV = 13604
.............*...*
optimization finished, #iter = 16227
obj = -10168.568345, rho = -0.124297
nSV = 13739, nBSV = 10297
Total nSV = 13739
............*....*
optimization finished, #iter = 16056
obj = -10173.709328, rho = -0.120681
nSV = 13724, nBSV = 10346
Total nSV = 13724
.............*...*
optimization finished, #iter = 16494
obj = -10137.262928, rho = -0.125899
nSV = 13696, nBSV = 10273
Total nSV = 13696
............*....*
optimization finished, #iter = 16436
obj = -10145.478510, rho = -0.125010
nSV = 13657, nBSV = 10270
Total nSV = 13657
...............*....*
optimization finished, #iter = 19745
obj = -12469.801091, rho = 0.116644
nSV = 16629, nBSV = 12680
.......*...*
optimization finished, #iter = 10022
obj = -5813.509811, rho = 0.159683
nSV = 8224, nBSV = 5872
Total nSV = 8224
.......*...*
optimization finished, #iter = 10021
obj = -5811.743678, rho = 0.156440
nSV = 8215, nBSV = 5870
Total nSV = 8215
.......*...*
optimization finished, #iter = 10024
obj = -5773.933883, rho = 0.147396
nSV = 8148, nBSV = 5843
Total nSV = 8148
.......*..*
optimization finished, #iter = 9975
obj = -5779.176524, rho = 0.161861
nSV = 8180, nBSV = 5832
Total nSV = 8180
.......*...*
optimization finished, #iter = 10145
obj = -5860.443981, rho = 0.169956
nSV = 8243, nBSV = 5930
Total nSV = 8243
.........*..*
optimization finished, #iter = 11754
obj = -7106.982782, rho = -0.160944
nSV = 9915, nBSV = 7194
...........*...*
optimization finished, #iter = 14703
obj = -9506.258820, rho = 0.064024
nSV = 12586, nBSV = 9698
Total nSV = 12586
...........*...*
optimization finished, #iter = 14597
obj = -9510.313090, rho = 0.078993
nSV = 12622, nBSV = 9677
Total nSV = 12622
...........*...*
optimization finished, #iter = 14471
obj = -9369.238985, rho = 0.078572
nSV = 12442, nBSV = 9536
Total nSV = 12442
...........*...*
optimization finished, #iter = 14749
obj = -9412.493944, rho = 0.074067
nSV = 12491, nBSV = 9580
Total nSV = 12491
...........*...*
optimization finished, #iter = 14843
obj = -9535.243397, rho = 0.063512
nSV = 12611, nBSV = 9713
Total nSV = 12611
.............*....*
optimization finished, #iter = 17377
obj = -11626.973159, rho = -0.074556
nSV = 15246, nBSV = 11883
.........................................................*...............*
optimization finished, #iter = 72767
obj = -47370.299280, rho = 0.353921
nSV = 56945, nBSV = 48737
Total nSV = 56945
........................................................*................*.*
optimization finished, #iter = 72670
obj = -47484.625067, rho = 0.337965
nSV = 57126, nBSV = 48836
Total nSV = 57126
.......................................................*................*
optimization finished, #iter = 71969
obj = -47334.308343, rho = 0.339156
nSV = 56971, nBSV = 48672
Total nSV = 56971
........................................................*...............*
optimization finished, #iter = 71827
obj = -47391.618465, rho = 0.333975
nSV = 56948, nBSV = 48809
Total nSV = 56948
.......................................................*.................*
optimization finished, #iter = 72356
obj = -47261.420207, rho = 0.350484
nSV = 56848, nBSV = 48628
Total nSV = 56848
...................................................................*..................*
optimization finished, #iter = 85509
obj = -58554.209099, rho = -0.341543
nSV = 70040, nBSV = 60272
.....................*......*
optimization finished, #iter = 27440
obj = -16439.884357, rho = 0.260019
nSV = 21868, nBSV = 16599
Total nSV = 21868
.....................*.......*
optimization finished, #iter = 28238
obj = -16289.139357, rho = 0.252810
nSV = 21785, nBSV = 16405
Total nSV = 21785
.....................*......*
optimization finished, #iter = 27621
obj = -16299.893485, rho = 0.260746
nSV = 21762, nBSV = 16456
Total nSV = 21762
.....................*.......*
optimization finished, #iter = 28030
obj = -16274.162267, rho = 0.263408
nSV = 21755, nBSV = 16414
Total nSV = 21755
.....................*.......*
optimization finished, #iter = 28399
obj = -16356.010280, rho = 0.255408
nSV = 21872, nBSV = 16494
Total nSV = 21872
..........................*.......*
optimization finished, #iter = 33579
obj = -20097.904618, rho = -0.256586
nSV = 26610, nBSV = 20302
..............*...*
optimization finished, #iter = 17902
obj = -9600.656982, rho = 0.498308
nSV = 13436, nBSV = 9681
Total nSV = 13436
..............*....*
optimization finished, #iter = 18391
obj = -9594.366332, rho = 0.486958
nSV = 13380, nBSV = 9648
Total nSV = 13380
..............*....*
optimization finished, #iter = 18563
obj = -9553.055073, rho = 0.501887
nSV = 13436, nBSV = 9598
Total nSV = 13436
..............*....*
optimization finished, #iter = 18684
obj = -9663.149956, rho = 0.492174
nSV = 13575, nBSV = 9723
Total nSV = 13575
.............*.....*
optimization finished, #iter = 18188
obj = -9568.416278, rho = 0.493615
nSV = 13389, nBSV = 9610
Total nSV = 13389
................*.....*
optimization finished, #iter = 21687
obj = -11803.249662, rho = -0.488933
nSV = 16297, nBSV = 11895
...........................*.........*
optimization finished, #iter = 36446
obj = -20627.047405, rho = 0.532717
nSV = 26656, nBSV = 21062
Total nSV = 26656
...........................*.........*
optimization finished, #iter = 36043
obj = -20487.262428, rho = 0.523555
nSV = 26457, nBSV = 20847
Total nSV = 26457
............................*........*
optimization finished, #iter = 36596
obj = -20503.906790, rho = 0.514934
nSV = 26590, nBSV = 20896
Total nSV = 26590
............................*........*
optimization finished, #iter = 36294
obj = -20540.298681, rho = 0.525415
nSV = 26577, nBSV = 20965
Total nSV = 26577
...........................*........*
optimization finished, #iter = 35583
obj = -20384.651119, rho = 0.525347
nSV = 26337, nBSV = 20819
Total nSV = 26337
................................*.........*
optimization finished, #iter = 41864
obj = -25288.945748, rho = -0.520944
nSV = 32410, nBSV = 25851
...................*......*
optimization finished, #iter = 25082
obj = -15754.753182, rho = 0.012534
nSV = 20630, nBSV = 16017
Total nSV = 20630
...................*.....*
optimization finished, #iter = 24695
obj = -15747.851252, rho = 0.007110
nSV = 20592, nBSV = 16033
Total nSV = 20592
...................*.....*.*
optimization finished, #iter = 25219
obj = -15690.506984, rho = 0.019775
nSV = 20563, nBSV = 15990
Total nSV = 20563
...................*......*
optimization finished, #iter = 25015
obj = -15821.241076, rho = 0.013504
nSV = 20684, nBSV = 16132
Total nSV = 20684
...................*.....*
optimization finished, #iter = 24755
obj = -15755.753931, rho = 0.012214
nSV = 20518, nBSV = 16067
Total nSV = 20518
Line search fails in two-class probability estimates
.......................*......*
optimization finished, #iter = 29555
obj = -19376.508928, rho = -0.013219
nSV = 25089, nBSV = 19768
....................*.....*
optimization finished, #iter = 25807
obj = -17171.535862, rho = 0.302137
nSV = 21393, nBSV = 17576
Total nSV = 21393
....................*.....*
optimization finished, #iter = 25973
obj = -17324.346663, rho = 0.289048
nSV = 21592, nBSV = 17721
Total nSV = 21592
....................*.....*
optimization finished, #iter = 25565
obj = -17118.063548, rho = 0.319646
nSV = 21362, nBSV = 17521
Total nSV = 21362
....................*.....*
optimization finished, #iter = 25542
obj = -17211.092997, rho = 0.311730
nSV = 21461, nBSV = 17618
Total nSV = 21461
...................*......*
optimization finished, #iter = 25447
obj = -17120.584721, rho = 0.303617
nSV = 21285, nBSV = 17523
Total nSV = 21285
.......................*.......*
optimization finished, #iter = 30924
obj = -21230.618873, rho = -0.302730
nSV = 26247, nBSV = 21727
..........................*.......*
optimization finished, #iter = 33827
obj = -21891.093390, rho = 0.277319
nSV = 27236, nBSV = 22557
Total nSV = 27236
.........................*........*.*
optimization finished, #iter = 33380
obj = -21950.235855, rho = 0.295592
nSV = 27273, nBSV = 22583
Total nSV = 27273
..........................*.......*
optimization finished, #iter = 33928
obj = -22126.339782, rho = 0.296677
nSV = 27486, nBSV = 22774
Total nSV = 27486
.........................*........*
optimization finished, #iter = 33344
obj = -21990.662982, rho = 0.286355
nSV = 27285, nBSV = 22694
Total nSV = 27285
..........................*........*
optimization finished, #iter = 34070
obj = -22103.043522, rho = 0.294623
nSV = 27509, nBSV = 22732
Total nSV = 27509
...............................*.........*
optimization finished, #iter = 40905
obj = -27165.327589, rho = -0.292001
nSV = 33510, nBSV = 27954
............*....*
optimization finished, #iter = 16285
obj = -10097.079942, rho = 0.267804
nSV = 13481, nBSV = 10323
Total nSV = 13481
............*....*
optimization finished, #iter = 16494
obj = -10108.825036, rho = 0.268010
nSV = 13453, nBSV = 10320
Total nSV = 13453
............*....*
optimization finished, #iter = 16160
obj = -10107.059330, rho = 0.276512
nSV = 13483, nBSV = 10327
Total nSV = 13483
............*....*.*
optimization finished, #iter = 16773
obj = -10082.075649, rho = 0.276881
nSV = 13488, nBSV = 10318
Total nSV = 13488
............*....*
optimization finished, #iter = 16551
obj = -10139.937530, rho = 0.280900
nSV = 13507, nBSV = 10340
Total nSV = 13507
...............*....*
optimization finished, #iter = 19674
obj = -12420.491849, rho = -0.268691
nSV = 16363, nBSV = 12707
.............*....*
optimization finished, #iter = 17318
obj = -10158.912587, rho = 0.212293
nSV = 13967, nBSV = 10369
Total nSV = 13967
.............*....*
optimization finished, #iter = 17783
obj = -10189.443070, rho = 0.220235
nSV = 13993, nBSV = 10413
Total nSV = 13993
.............*....*
optimization finished, #iter = 17716
obj = -10177.815435, rho = 0.207717
nSV = 13990, nBSV = 10391
Total nSV = 13990
.............*....*
optimization finished, #iter = 17309
obj = -10119.908090, rho = 0.201328
nSV = 13928, nBSV = 10334
Total nSV = 13928
.............*....*
optimization finished, #iter = 17735
obj = -10148.410934, rho = 0.206899
nSV = 13954, nBSV = 10331
Total nSV = 13954
................*....*
optimization finished, #iter = 20715
obj = -12456.383306, rho = -0.208304
nSV = 16885, nBSV = 12766
........*..*
optimization finished, #iter = 10564
obj = -5918.955578, rho = -0.101314
nSV = 8446, nBSV = 5969
Total nSV = 8446
........*..*
optimization finished, #iter = 10602
obj = -5880.562487, rho = -0.086330
nSV = 8396, nBSV = 5929
Total nSV = 8396
........*..*
optimization finished, #iter = 10562
obj = -5955.982849, rho = -0.103543
nSV = 8466, nBSV = 5972
Total nSV = 8466
........*...*.*
optimization finished, #iter = 11295
obj = -5934.249300, rho = -0.081161
nSV = 8433, nBSV = 5951
Total nSV = 8433
........*..*
optimization finished, #iter = 10814
obj = -5992.089381, rho = -0.072483
nSV = 8535, nBSV = 6022
Total nSV = 8535
.........*...*
optimization finished, #iter = 12821
obj = -7260.402750, rho = 0.087180
nSV = 10194, nBSV = 7295
Total nSV = 193321
SVC模型保存完成
KNN模型保存完成
RF模型保存完成

进程已结束，退出代码为 0
